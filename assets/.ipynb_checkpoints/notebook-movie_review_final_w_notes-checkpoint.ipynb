{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "# http://www.numpy.org/\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "# https://www.nltk.org/\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download movie reviews and stopwords from Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\scook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\scook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"\n",
    "    Get movie review data\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    y_labels = []\n",
    "    # Extract categories\n",
    "    for cat in movie_reviews.categories():\n",
    "        # for files in each cateogry    \n",
    "        for fileid in movie_reviews.fileids(cat):\n",
    "            # Get the words in that category\n",
    "            words = list(movie_reviews.words(fileid))\n",
    "            # the resulting dataset stores sentences\n",
    "            sentences = \" \".join(word for word in words)\n",
    "            dataset.append((sentences))\n",
    "            y_labels.append(cat)\n",
    "    return dataset,y_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = get_data()\n",
    "target_labels = ['neg','pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data structures for stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8047\n"
     ]
    }
   ],
   "source": [
    "# count of words that have been negated \n",
    "count = 0\n",
    "# container for sentences that have undergone first transformation\n",
    "secondsents = []\n",
    "# traverse sentences in the dataset\n",
    "for sent in dataset:\n",
    "    final_words = []\n",
    "    # split sentence back into words\n",
    "    words = sent.split(\" \")\n",
    "    # negation flag and negation words to look at\n",
    "    negate = False\n",
    "    negate_words = ['no','not']\n",
    "    # go through all the words in the sentence to see if the sentence is negated\n",
    "    for word in words:\n",
    "        # if the negate flag is true append the current word with Not_, increase the negation count, and reset the flag\n",
    "        if negate:\n",
    "            word = 'Not_' + word\n",
    "            count += 1\n",
    "            print(word)\n",
    "            negate = False\n",
    "        # if the flag isn't set and the current word isn't a negation add it to the final word list\n",
    "        if word not in negate_words:\n",
    "            final_words.append(word)\n",
    "        # if the negation flag was false and the word was a negation flip the flag to true\n",
    "        else:\n",
    "            negate = True\n",
    "    # overwrite words with modified words\n",
    "    words = final_words\n",
    "    # remove stopwords and punctuation from words list\n",
    "    words = [word for word in words if word.lower() not in stopwords and word.lower() not in punctuation]\n",
    "    # overwrite sentence with sentence constructed from new words list\n",
    "    sent = \" \".join(word for word in words)\n",
    "    # put new sentence into data structure\n",
    "    secondsents.append(sent)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# container for sentences that have undergone second transformation(bigram detection)\n",
    "thirdsents = []\n",
    "for sent in secondsents:\n",
    "    print(sent)\n",
    "    words = sent.split(\" \")\n",
    "    # find bigrams within current sentence by using BigramCollationFinder package and choosing the top 400 bigrams by frequency\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(BigramAssocMeasures.raw_freq,400)\n",
    "    # combine bigrams into single string and add to word list\n",
    "    for bigram in bigrams:\n",
    "        BG = bigram[0] + \"_\" + bigram[1]\n",
    "        words.append(BG)\n",
    "    # create new sentence w/ bigrams included\n",
    "    sent = \" \".join(word for word in words)\n",
    "    print(sent+\"\\n\\n\")\n",
    "    thirdsents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Count Vectorizer: Convert a collection of text documents to a matrix of token counts\n",
    "tfid: Transform a count matrix to a normalized tf or tf-idf representation. Tf means term-frequency while \n",
    "      tf-idf means term-frequency times inverse document-frequency.\n",
    "MultinomialNB: Naive Bayes classifier for multinomial models\n",
    "               The multinomial Naive Bayes classifier is suitable for classification with discrete features \n",
    "               (e.g., word counts for text classification). The multinomial distribution normally requires \n",
    "               integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n",
    "'''\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Stochastic Gradient Descent (SGD): is a simple yet very efficient approach \n",
    "to discriminative learning of linear classifiers under convex loss functions\n",
    "such as (linear) Support Vector Machines and Logistic Regression. Even\n",
    "though SGD has been around in the machine learning community for a long time,\n",
    "it has received a considerable amount of attention just recently in the context of large-scale learning.\n",
    "'''\n",
    "text_sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, random_state=42)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_model_NB(data,labels):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=5)\n",
    "    text_clf.fit(X_train,y_train)\n",
    "    predicted = text_clf.predict(X_test)\n",
    "    print(\"Accuracy is: %0.4f\"%np.mean(predicted == y_test) + \"%\")\n",
    "    print(metrics.classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_test_model_SGD(data,labels):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=5)\n",
    "    text_sgd.fit(X_train,y_train)\n",
    "    predicted = text_sgd.predict(X_test)\n",
    "    print(\"Accuracy is: %0.4f\"%np.mean(predicted == y_test) + \"%\")\n",
    "    print(metrics.classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8030%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.76      0.88      0.82       329\n",
      "        pos       0.86      0.73      0.79       331\n",
      "\n",
      "avg / total       0.81      0.80      0.80       660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_test_model_NB(dataset,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8197%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.80      0.85      0.83       329\n",
      "        pos       0.84      0.79      0.81       331\n",
      "\n",
      "avg / total       0.82      0.82      0.82       660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_test_model_NB(secondsents,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8061%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.85      0.74      0.79       329\n",
      "        pos       0.77      0.87      0.82       331\n",
      "\n",
      "avg / total       0.81      0.81      0.81       660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_test_model_NB(thirdsents,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8424%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.85      0.84       329\n",
      "        pos       0.85      0.84      0.84       331\n",
      "\n",
      "avg / total       0.84      0.84      0.84       660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_test_model_SGD(secondsents,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8303%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.82      0.84      0.83       329\n",
      "        pos       0.84      0.82      0.83       331\n",
      "\n",
      "avg / total       0.83      0.83      0.83       660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_test_model_SGD(thirdsents,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(secondsents, labels, test_size=0.33, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1340, 34733)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_train_counts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1340, 34733)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8196969696969697"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = clf.predict(X_test_tfidf)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time also well good story life like one movie film\n"
     ]
    }
   ],
   "source": [
    "feature_names = list(count_vect.get_feature_names())\n",
    "top10 = np.argsort(clf.coef_[0])[-10:]\n",
    "#print(top10)\n",
    "print(\" \".join(feature_names[j] for j in top10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
